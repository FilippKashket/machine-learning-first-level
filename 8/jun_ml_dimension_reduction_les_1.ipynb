{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "jun_ml_dimension_reduction_les-1.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hy9tuT-cN6r8",
        "colab_type": "text"
      },
      "source": [
        "В этом модуле мы поговорим про снижение размерности: зачем это нужно, какие математические методы стоят за этой задачей и какие алгоритмы в библиотеке sklearn нам помогут."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUqDJ7DDN6sA",
        "colab_type": "text"
      },
      "source": [
        "# Урок 1. Постановка задачи понижения размерности\n",
        "\n",
        "Алгоритмы машинного обучения работают с выборками, в которых могут быть десятки и даже сотни признаков. Однако, не все признаки могут быть одинаково полезны. Например, если мы занимается задачей прогнозирования времени нервного срыва, то некоторые фичи у нас будут независимы друг от друга (например, \"угрюмость\" настроения и артериальное давление), в то время как другие переменные будут связаны друг с другом т.е. зависимы: например, на картинке мы видим, что фича *угрюмость настроения* зависит линейно от фичи *количество часов сна* - очевидно, что чем меньше человек спит, тем больше угрюмость.\n",
        "\n",
        "![sleepy_mood](https://248006.selcdn.ru/public/DS.%20Block%202.%20M8/sleepy_mood.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnU_zAsLN6sC",
        "colab_type": "text"
      },
      "source": [
        "Интуитивно кажется, что из двух зависимых переменных можно оставить одну, т.к. в данном случае \"связанные\" переменные не добавляют качественно новой информации - мы можем в модели использовать только количество часов сна, потому что \"угрюмость\" всё равно связана с этой фичёй. \n",
        "\n",
        "Так возникает задача снижения размерности - хотим заменить несколько таких \"связанные\" фичей на одну. Без потери качества.\n",
        "\n",
        "Карлом Пирсоном в 1901 был придуман Principal Component Analysis, или метод главных компонент. Пирсон решал задачу аппроксимации (приближения) экспериментальных данных линейными преобразованиями, т.е. прямыми. Пирсон пытался перейти от двумерной задачи к одномерной. Ниже картинка из оригинальной работы, на которой демонстрируется главная компонента, которая представляет собой прямую линию\n",
        "\n",
        "![главная компонента](https://248006.selcdn.ru/public/DS.%20Block%202.%20M8/PearsonFig.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oaLsO7HN6sE",
        "colab_type": "text"
      },
      "source": [
        "**Метод PCA** ( principal component analysis) позволяет снизить число переменных, выбрав самые изменчивые из них . Интуиция тут такая: чем сильнее изменяется переменная (чем больше у неё дисперсия, такую переменную называют \"вариативной\") - тем больше она содержит информации. *Главные компоненты* - это новые переменные, которые представляют собой линейные преобразования от исходных фичей. Например, на картинке с предыдущего слайда есть прямая и вдоль этой прямой данные изменяются *сильнее всего*.\n",
        "\n",
        "Например, есть переменные $x_1$ и $x_2$. Главные компоненты - это просто преобразования исходных координат. Выглядеть они могут, например так (**сильно вникать не нужно, это всего лишь пример**):\n",
        "$$\n",
        "\\left\\{\n",
        "\\begin{array}{c}\n",
        "p_1 = \\frac{1}{\\sqrt{2}}x_1 + \\frac{1}{\\sqrt{2}}x_2 \\\\\n",
        "p_2 = -\\frac{1}{2}x_1 - \\frac{\\sqrt{3}}{2}x_2\n",
        "\\end{array}\n",
        "\\right.\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gJzOHTSN6sF",
        "colab_type": "text"
      },
      "source": [
        "Пирсон предложил две формулы, которые по сути своей поворачивают систему координат так, чтобы новые оси координат указывали направление, где дисперсия в облаке точек максимальна - см. пример на картинке:\n",
        "\n",
        "![pca_axes_transform](https://248006.selcdn.ru/public/DS.%20Block%202.%20M8/pca_axes_transform.png)\n",
        "\n",
        "Мы берем одну из этих формул (всегда первую - потому что из-за математической магии первая главная компонента \"забирает в себя\" большую часть дисперсии) и пересчитываем по ней каждую точку, снижая размерность на единицу. Теперь у нас на одну фичу меньше! Как же нам это поможет?\n",
        "\n",
        "Понижение размерности применяется для:\n",
        "\n",
        "* визуализации многомерных данных на плоскости. Многомерные датасеты иногда проще воспринять графически\n",
        "* подготовки данных для алгоритмов более высокого уровня. Мы знаем, что линейная регрессия плохо справляется с выбросами а PCA как раз сожмёт выбросы в линию (для двумерного случая)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGD680NFN6sH",
        "colab_type": "text"
      },
      "source": [
        "В этом уроке мы поговорили о том, зачем нужно понижать размерность пространства фичей. Мы обсудили такой метод понижения размерности, как PCA. В следующем уроке мы посмотрим, как применять реализацию PCA \"из коробки\"."
      ]
    }
  ]
}