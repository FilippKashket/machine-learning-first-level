{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Ca6A52yRgN8"
   },
   "source": [
    "# Дополнительные техники классификации.\n",
    "# Урок 1. Boosting.\n",
    "\n",
    "Ранее мы уже упоминали, что существует такое понятие как *ансамбли алгоритмов* - метод, использующий одновременно несколько обучающих алгоритмов для получения лучшего результата по сравнению с результатами каждого из алгоритмов в отдельности. Этот метод базируется на интуитивном соображении, пришедшем из статистики, согласно которому усреднение результатов наблюдений может дать более устойчивую и надежную оценку, так как ослабляет влияние различных флуктуаций в отдельных измерениях.\n",
    "\n",
    "Большинство приемов в ансамблировании направлено на то, чтобы ансамбль был достаточно разнообразным, тогда ошибки одних алгоритмов будут компенсироваться корректной работой других.\n",
    "\n",
    "Алгоритмы, из которых состоит ансамбль, называются базовыми алгоритмами (base learners), а алгоритм, который комбинирует полученные ответы - мета-алгоритмом (meta-estimator). По сути, при построении ансамбля:\n",
    "\n",
    "1. повышают качество базовых алгоритмов\n",
    "2. повышают разнообразие базовых алгоритмов за счет:\n",
    "    - варьирования обучающей выборки (каждый алгоритм использует различные подвыборки исходной выборки)\n",
    "    - варьирования признаков (каждый алгоритм использует свой рандомный набор признаков)\n",
    "    - варьирования моделей (используются различные модели)\n",
    "    - варьирования в модели (использование различных гиперпараметров в рамках одного алгоритма)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SSAS0EfMRgN-"
   },
   "source": [
    "Вообще, существует достаточно много моделей ансамблирования: \n",
    "- голосование/усреднение ответов по независимым алгоритмам (например, *случайный лес*)\n",
    "- кодировка целевых значений и сведение решения задачи к решению нескольких задач (например, каждый класс может кодироваться бинарным кодом, а решение задачи многоклассовой классификации сводиться к решению нескольких задач бинарной классификации - метод *ECOC*)\n",
    "- построение суммы нескольких алгоритмов, где каждый следующий алгоритм строится с учетом ошибок предыдущих (*бустинг*)\n",
    "- построение метапризнаков - ответов базовых алгоритмов на выборке, обучение на них мета-алгоритма (*стекинг*)\n",
    "- однородные ансамбли (например, *нейронные сети*)\n",
    "\n",
    "Рассмотрим одни из самые популярных методов: **бустинг** и **стекинг**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5npQikQYRgN_"
   },
   "source": [
    "### Boosting (Бустинг)\n",
    "\n",
    "Главная идея бустинга - последовательное построение базовых алгоритмов, каждый из которых обучается, используя информацию об ошибках, сделанных на предыдущем этапе. Т.е. каждый следующий алгоритм мы строим таким образом, чтобы он исправлял ошибки предыдущих и повышал качество всего ансамбля. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uOw5zxb1RgOA"
   },
   "source": [
    "Первый успешный вариант бустинга - **AdaBoost (Adaptive Boosting)**. Основная идея состоит в том, что каждый следующий алгоритм строится, опираясь на объекты, неверно классифицированные предыдущими алгоритмами. Это достигается за счет того, что при построении каждого следующего алгоритма меняются веса объектов: объекты, которые классифицировались неверно, приобретают больший вес, а объекты, которые были определены правильно, - меньший. \n",
    "\n",
    "Подробно разбирать этот алгоритм не имеет смысла, так как его вытеснил градиентный бустинг: оказалось, что AdaBoost является его частной вариацией (что будет показано позднее).\n",
    "\n",
    "Наглядно алгоритм AdaBoost можно представить таким образом: пусть у нас имеются два класса, которые необходимо разделить, а в качестве базовых алгоритмов мы используем деревья решений глубины 1 (т.е. одно дерево может разделить данные только одной прямой линией по горизонтали или вертикали).\n",
    "\n",
    "![](https://248006.selcdn.ru/public/DS.%20Block%202.%20M9/AdaBoost_1.png) \n",
    "\n",
    "После обучения первого дерева мы получаем такой результат:\n",
    "\n",
    "![](https://248006.selcdn.ru/public/DS.%20Block%202.%20M9/AdaBoost_2.png)\n",
    "\n",
    "Видим, что веса объектов, на которых ошибся первый базовый алгоритм, увеличились, а веса тех, что были классифицированы правильно, уменьшились (на графике это передается с помощью размера точек).\n",
    "\n",
    "После обучения второго и третьего деревьев соответственно получаем:\n",
    "\n",
    "![](https://248006.selcdn.ru/public/DS.%20Block%202.%20M9/AdaBoost_3.png)\n",
    "![](https://248006.selcdn.ru/public/DS.%20Block%202.%20M9/AdaBoost_4.png)\n",
    "\n",
    "После того, как все базовые алгоритмы были обучены, AdaBoost строит финальную модель  путем суммирования произведений веса каждого дерева на его предсказание (т.е. на то, как дерево разделило классы).\n",
    "\n",
    "![](https://248006.selcdn.ru/public/DS.%20Block%202.%20M9/AdaBoost_5.png)\n",
    "\n",
    "Как видим, в таком варианте полученное разделение соответствует идеальной классификации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UiV6UcBdRgOA"
   },
   "source": [
    "**GradientBoosting**. Как понятно из названия, основан на понятии градиента: не вдаваясь в математические подробности, можно сказать, что градиент - это вектор, который направлен в сторону наискорейшего возрастания функции. Соответственно, антиградиент направлен в сторону наискорейшего убывания функции. \n",
    "\n",
    "Допустим, у нас есть выборка, состоящая из признаков *x* и целевых переменных *y*. Идея градиентного бустинга состоит в том, чтобы уменьшить ошибку предсказания классификатора $\\hat{y}$, т.е. уменьшить разность $(y - \\hat{y})$, чтобы она была как можно ближе к нулю. Для этой цели используется т.н. функция потерь, т.е. функция, которая характеризует потери при неправильном принятии решений (одну из них - логистическую функцию потерь - мы упоминали в одном из предыдущих уроков). Если обозначить ее как $L$, то искомую наименьшую разность можно переписать в виде $arg min_{\\hat{y}}\\ L(y, \\hat{y})$, а поиск этой наименьшей разности производится самым простым и часто используемым методом - градиентным спуском.\n",
    "\n",
    "Заметим, что, исходя из использования градиента, функция потерь должна быть дифференцируемой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8U9MOuUGRgOB"
   },
   "source": [
    "Теперь разберемся с тем, какие функции потерь мы можем использовать в задаче классификации. Так как оптимизировать сами метки класса довольно затруднительно, обычно используют пороговые значения, когда $\\hat{y} \\in \\{-1, 1\\}$. Наиболее известные функции потерь в классификации:\n",
    "- Logistic loss или логистическая функция потерь: $L(y, \\hat{y}) = log(1 + exp(-y \\hat{y}))$ - самая используемая функция потерь в бинарной классификации;\n",
    "- AdaBoost loss: $L(y, \\hat{y}) = exp(-y \\hat{y})$. Здесь нужно вспомнить про алгоритм AdaBoost: так получилось, что он эквивалентен методу градиентного бустинга с этой функцией потерь. Эта функция имеет более жесткий штраф по отношению к ошибкам классификации и используется реже.\n",
    "\n",
    "В качестве базовых алгоритмов здесь обычно используются решающие деревья. При этом решающие деревья выбираются неглубокими: глубина варьируется от 2 до 7. Этого вполне достаточно, чтобы восстановить сложные закономерности с помощью бустинга."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "egWmmxx_RgOC"
   },
   "source": [
    "Еще одна эффективная вариация градиентного бустинга на деревьях решений - **XGBoost (eXtreme Gradient Boosting)**. Модель представлена отдельной одноименной библиотекой. По сравнению с классическим градиентным бустингом, в данной реализации больше возможностей для регуляризации базовых решающих деревьев и задачи бустинга в целом. Выделяют три группы параметров:\n",
    "- общие параметры, отвечающие за выбор базового алгоритма и распараллеливание;\n",
    "- параметры базового алгоритма решающих деревьев: скорость обучения (чем ниже, тем больше итераций нужно для обучения модели с хорошим качеством), максимальная глубина дерева, минимальное необходимое число объектов в каждом листе, доля выборки для обучения каждого дерева, доля признаков для каждого дерева, коэффициенты перед L1- и L2-регуляризаторами функции потерь;\n",
    "- параметры задачи обучения: используемая при обучении функция потерь, метрика качества на валидационной выборке, параметр воспрозводимости."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x0aFHoVMRgOC"
   },
   "source": [
    "Теперь перейдем к практике. Библиотека sklearn предоставляет нам готовую реализацию алгоритмов AdaBoost и GradientBoosting, а вот XGBoost придется импортировать из другой библиотеки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TlePQZhxRgOD"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SEJYtdUjRgOG"
   },
   "source": [
    "Все рассмотренные методы являются композициями алгоритмов. Логично предположить, что их качество будет зависеть от количества базовых алгоритмов, над которыми они строятся. Давайте на примере этих алгоритмов проанализируем, как изменяется качество алгоритма в зависимости от количества деревьев, на которых он построен.\n",
    "\n",
    "Для демонстрации используем уже преобразованные в одном из предыдущих домашних заданий данные задачи из Kaggle - Titanic.\n",
    "\n",
    "Вспомним, как они выглядят."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3xWDpBICRgOH",
    "outputId": "7a096549-53e4-4a07-8414-292bbe195ec0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass  Sex   Age  SibSp  Parch     Fare  Embarked\n",
       "0         0       3    1  22.0      1      0   7.2500         1\n",
       "1         1       1    0  38.0      1      0  71.2833         2\n",
       "2         1       3    0  26.0      0      0   7.9250         1\n",
       "3         1       1    0  35.0      1      0  53.1000         1\n",
       "4         0       3    1  35.0      0      0   8.0500         1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "titanic = pd.read_csv('titanic.csv')\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X24ZZenaRgOL"
   },
   "source": [
    "Сразу отделим целевую переменную Survived. На тренировочный и тестовый датасеты данные разделять не будем - сделаем проверку производительности на кросс-валидации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wp1XCWAGRgOM"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0708</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16.7000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26.5500</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>31.2750</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8542</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>29.1250</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>18.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0292</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35.5000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>31.3875</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>263.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8792</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11.5000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.9292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>69.5500</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13.8583</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50.4958</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.5000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>52.5542</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>24.0000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.8458</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>83.1583</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.5167</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.5000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0500</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>29.1250</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pclass  Sex   Age  SibSp  Parch      Fare  Embarked\n",
       "0         3    1  22.0      1      0    7.2500         1\n",
       "1         1    0  38.0      1      0   71.2833         2\n",
       "2         3    0  26.0      0      0    7.9250         1\n",
       "3         1    0  35.0      1      0   53.1000         1\n",
       "4         3    1  35.0      0      0    8.0500         1\n",
       "5         3    1  29.0      0      0    8.4583         3\n",
       "6         1    1  54.0      0      0   51.8625         1\n",
       "7         3    1   2.0      3      1   21.0750         1\n",
       "8         3    0  27.0      0      2   11.1333         1\n",
       "9         2    0  14.0      1      0   30.0708         2\n",
       "10        3    0   4.0      1      1   16.7000         1\n",
       "11        1    0  58.0      0      0   26.5500         1\n",
       "12        3    1  20.0      0      0    8.0500         1\n",
       "13        3    1  39.0      1      5   31.2750         1\n",
       "14        3    0  14.0      0      0    7.8542         1\n",
       "15        2    0  55.0      0      0   16.0000         1\n",
       "16        3    1   2.0      4      1   29.1250         3\n",
       "17        2    1  29.0      0      0   13.0000         1\n",
       "18        3    0  31.0      1      0   18.0000         1\n",
       "19        3    0  27.0      0      0    7.2250         2\n",
       "20        2    1  35.0      0      0   26.0000         1\n",
       "21        2    1  34.0      0      0   13.0000         1\n",
       "22        3    0  15.0      0      0    8.0292         3\n",
       "23        1    1  28.0      0      0   35.5000         1\n",
       "24        3    0   8.0      3      1   21.0750         1\n",
       "25        3    0  38.0      1      5   31.3875         1\n",
       "26        3    1  29.0      0      0    7.2250         2\n",
       "27        1    1  19.0      3      2  263.0000         1\n",
       "28        3    0  27.0      0      0    7.8792         3\n",
       "29        3    1  29.0      0      0    7.8958         1\n",
       "..      ...  ...   ...    ...    ...       ...       ...\n",
       "861       2    1  21.0      1      0   11.5000         1\n",
       "862       1    0  48.0      0      0   25.9292         1\n",
       "863       3    0  27.0      8      2   69.5500         1\n",
       "864       2    1  24.0      0      0   13.0000         1\n",
       "865       2    0  42.0      0      0   13.0000         1\n",
       "866       2    0  27.0      1      0   13.8583         2\n",
       "867       1    1  31.0      0      0   50.4958         1\n",
       "868       3    1  29.0      0      0    9.5000         1\n",
       "869       3    1   4.0      1      1   11.1333         1\n",
       "870       3    1  26.0      0      0    7.8958         1\n",
       "871       1    0  47.0      1      1   52.5542         1\n",
       "872       1    1  33.0      0      0    5.0000         1\n",
       "873       3    1  47.0      0      0    9.0000         1\n",
       "874       2    0  28.0      1      0   24.0000         2\n",
       "875       3    0  15.0      0      0    7.2250         2\n",
       "876       3    1  20.0      0      0    9.8458         1\n",
       "877       3    1  19.0      0      0    7.8958         1\n",
       "878       3    1  29.0      0      0    7.8958         1\n",
       "879       1    0  56.0      0      1   83.1583         2\n",
       "880       2    0  25.0      0      1   26.0000         1\n",
       "881       3    1  33.0      0      0    7.8958         1\n",
       "882       3    0  22.0      0      0   10.5167         1\n",
       "883       2    1  28.0      0      0   10.5000         1\n",
       "884       3    1  25.0      0      0    7.0500         1\n",
       "885       3    0  39.0      0      5   29.1250         3\n",
       "886       2    1  27.0      0      0   13.0000         1\n",
       "887       1    0  19.0      0      0   30.0000         1\n",
       "888       3    0  27.0      1      2   23.4500         1\n",
       "889       1    1  26.0      0      0   30.0000         2\n",
       "890       3    1  32.0      0      0    7.7500         3\n",
       "\n",
       "[891 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = titanic.Survived\n",
    "data = titanic.drop(columns='Survived')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e4FoYRCgRgOO"
   },
   "source": [
    "Зададим количество деревьев в алгоритме: 1, а далее от 10 до 100 с шагом 10. В алгоритме за этот гиперпараметр отвечает n_estimtors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wtLnPhCURgOO"
   },
   "outputs": [],
   "source": [
    "trees = [1] + list(range(10, 100, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ObWi2FWsRgOR"
   },
   "source": [
    "Теперь последовательно обучим алгоритмы AdaBoost, GradientBoosting и XGBoost и проверим на трехкратной кросс-валидации, как изменяется точность классификаторов в зависимости от количества базовых алгоритмов. Заодно посмотрим на время обучения каждого из алгоритмов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gzQ3gryFRgOR",
    "outputId": "669a1be7-9bb1-4cb1-a164-25228ab1d144"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.38 s, sys: 6.71 ms, total: 2.39 s\n",
      "Wall time: 2.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ada_scoring = []\n",
    "for tree in trees:\n",
    "    ada = AdaBoostClassifier(n_estimators=tree)\n",
    "    score = cross_val_score(ada, data, targets, scoring='roc_auc', cv=3)\n",
    "    ada_scoring.append(score)\n",
    "ada_scoring = np.asmatrix(ada_scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W4yTAyzMRgOU",
    "outputId": "12a1c9d1-ac44-4419-c91f-773681ee2c5d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.77164222, 0.78587863, 0.7430975 ],\n",
       "        [0.79997124, 0.85557473, 0.86624005],\n",
       "        [0.80560349, 0.85092513, 0.88447896],\n",
       "        [0.79925223, 0.84766561, 0.88562937],\n",
       "        [0.80236794, 0.84502924, 0.87553926],\n",
       "        [0.80059438, 0.8420094 , 0.87553926],\n",
       "        [0.80040265, 0.84112262, 0.88050043],\n",
       "        [0.80136133, 0.84210526, 0.88203432],\n",
       "        [0.80299108, 0.84385486, 0.87340619],\n",
       "        [0.8038539 , 0.84450197, 0.87824753]])"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KN6rd3yiRgOW",
    "outputId": "257daac9-9f20-453a-d7f4-266416411056"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.28 s, sys: 3.74 ms, total: 1.28 s\n",
      "Wall time: 1.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "gbc_scoring = []\n",
    "for tree in trees:\n",
    "    gbc = GradientBoostingClassifier(n_estimators=tree)\n",
    "    score = cross_val_score(gbc, data, targets, scoring='roc_auc', cv=3)\n",
    "    gbc_scoring.append(score)\n",
    "gbc_scoring = np.asmatrix(gbc_scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2LpKEL7GRgOY",
    "outputId": "ff661fa8-45e4-45bc-b5a4-848e04d50d9b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.81430352, 0.84991851, 0.87491612],\n",
       "        [0.81763493, 0.87117726, 0.88677979],\n",
       "        [0.83158374, 0.87364586, 0.88179465],\n",
       "        [0.81703576, 0.8781996 , 0.8825616 ],\n",
       "        [0.81777874, 0.87407727, 0.88186655],\n",
       "        [0.81451922, 0.87103346, 0.88730707],\n",
       "        [0.81399195, 0.86930783, 0.88349631],\n",
       "        [0.81003739, 0.87017065, 0.88117151],\n",
       "        [0.80982169, 0.87146486, 0.87635414],\n",
       "        [0.81380021, 0.87192024, 0.87496405]])"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc_scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lW0xWu9bRgOb",
    "outputId": "99ff3f6f-948b-488d-d8b4-a87ef18ba995"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 655 ms, sys: 7.99 ms, total: 663 ms\n",
      "Wall time: 659 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "xgb_scoring = []\n",
    "for tree in trees:\n",
    "    xgb = XGBClassifier(n_estimators=tree)\n",
    "    score = cross_val_score(xgb, data, targets, scoring='roc_auc', cv=3)\n",
    "    xgb_scoring.append(score)\n",
    "xgb_scoring = np.asmatrix(xgb_scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QreZ7sKZRgOd",
    "outputId": "8ba4d25a-db8c-44c8-eeb3-9c83cafe03e7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.82019941, 0.84713834, 0.87673761],\n",
       "        [0.81288946, 0.87072189, 0.88711533],\n",
       "        [0.82842009, 0.87208801, 0.88359218],\n",
       "        [0.82504074, 0.87206404, 0.88809798],\n",
       "        [0.82707794, 0.871369  , 0.88613268],\n",
       "        [0.82616719, 0.87201611, 0.88100374],\n",
       "        [0.82401016, 0.87053015, 0.88337647],\n",
       "        [0.82139776, 0.86834915, 0.88311284],\n",
       "        [0.82264404, 0.86887643, 0.88618062],\n",
       "        [0.82329115, 0.86856485, 0.88618062]])"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y8dgXHzWRgOf"
   },
   "source": [
    "На первый взгляд алгоритмы работают практически идентично. Теперь посмотрим на графике, так ли это. Обратите внимание на то, что алгоритм XGBoost работает в 2 раза быстрее, чем GradienBoosting, и почти в 4 раза быстрее, чем AdaBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V9Ve-rGBRgOg",
    "outputId": "84845941-ca6d-4f90-a524-f47f9ea3b865"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fef77ed94e0>"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(trees, ada_scoring.mean(axis=1), label='AdaBoost')\n",
    "plt.plot(trees, gbc_scoring.mean(axis=1), label='GradientBoosting')\n",
    "plt.plot(trees, xgb_scoring.mean(axis=1), label='XGBoost')\n",
    "plt.grid(True)\n",
    "plt.xlabel('trees')\n",
    "plt.ylabel('auc score')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l1k9jOcuRgOi"
   },
   "source": [
    "Видим ожидаемую картину: при одном дереве качество работы алгоритма было ниже, чем с увеличением количества деревьев, однако все модели показывают наиболее высокое качество примерно у отметки в 20 базовых деревьев. Вообще, в бустинге увеличение числа деревьев не всегда приводит к улучшению качества решения на тестовых данных. Число деревьев, при котором качество алгоритма максимально, зависит от темпа обучения: чем меньше темп, тем больше деревьев обычно нужно (отметим, что зависимость нелинейная).\n",
    "\n",
    "Также, как видно по графику, алгоритм XGBoost работает не только быстрее, но и несколько лучше. Это достигается за счет того, что у него больше преднастроенных гиперпараметров и он лучше оптимизирован."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "jun_ml_extra_tech_boost.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
